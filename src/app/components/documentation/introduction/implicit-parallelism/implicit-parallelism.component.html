<div id="implicitParallelism">
<h3>Implicit Parallelism</h3>

<p>HPC was initially embraced by research scientists to execute various applications  requiring high computational power that a single computer was unable to provide. HPC ensures a significant performance increase due to faster hardware, higher memory capacity, optimisation, and the use of parallel computing. For instance, by making use of an HPC cluster, one can take advantage of multiple cores by running several instances of a program at once or using a parallel version of the program.</p>
<p>Implicit parallelism is the subset of parallel computing where the programmer is not expected to define how computation is parallelized. SciFlow caters for such implicit parallelism.</p>
<p>With the use of the Python parallel scripting library <a href="https://parsl-project.org/">Parsl</a>, you can easily configure the framework to run parallely. Parsl augments Python with simple, scalable, and flexible constructs for encoding parallelism. You can annotate Python functions to specify opportunities for concurrent execution.</p>
<p>Lets take a look at how Parsl provides parallel execution with a simple example. A program defines two Parsl annotated  functions, app_double and app_sum. It then makes four calls to the app_double app and one call to the app_sum app; these execute concurrently, synchronized by mapped_result variable. The following figure shows the resulting task graph.</p>

<img src="../../../assets/png/MapReduce.jpg" alt="Smiley face" class="center" width="500px">

<p>Within the framework we have provided configurations for running on,</p>

<ul>
  <li>Threads on your local machine</li>
  <li>Cores on your local machine</li>
  <li>On an Ad Hoc Cluster</li>
</ul>

<p>Explore more cloud, cluster and supercomputer configurations on the Parsl website. </p>
<p><a href="https://parsl.readthedocs.io/en/stable/userguide/configuring.html">https://parsl.readthedocs.io/en/stable/userguide/configuring.html</a></p>
<p>Thereâ€™s absolutely no requirement for you to bother with MPI codes in order to parallelize across an HPC platform. You just need to load the configuration!  </p>
</div>
