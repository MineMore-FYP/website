<p><a (click)="goTo('workflow')">What is a workflow?</a></p>
<p><a (click)="goTo('dynamicWF')">Dynamic Workflows</a></p>
<p><a (click)="goTo('composability')">Composability</a></p>
<p><a (click)="goTo('implicitParallelism')">Implicit Parallelism</a></p>
<br>

<div id="workflow">
<h3>What is a workflow?</h3>
<p>A Workflow is the definition, execution and automation of a process where tasks, information or documents
are passed from one participant to another for action, according to a set of procedural rules.</p>

<img src="../../../assets/png/wf.png" alt="Smiley face" class="center" width="500px">

<p>A workflow could be viewed as a graph. However, unlike a Directed Acyclic Graph, workflows permit applications
  to be defined as loops. </p>

<p>Contemporary scientific applications often require large-scale, multiple-simulation,
  complex pipelines in order to facilitate new knowledge discovery. Therefore, workflows have emerged
  as a paradigm providing much needed simplicity in defining complex distributed scientific applications.
  Such scientific workflows enable scientists to plug-in any scientific data source or module, inspect and
  visualize knowledge.</p>
<p>The workflow approach for scientific problem solving provides the following benefits; </p>

<ul>
  <li>Provide a declarative way for specifying the high-level logic of an application, hiding the low- level details (<b>abstraction</b>)</li>
  <li><b>Integrate</b> existing software modules, datasets, and services in complex compositions</li>
  <li>Ability to store and retrieve workflow components for modifications or re-execution (<b>reusability</b>)</li>
</ul>

<p>SciFlow provides such workflows, allowing you to express complex scientific processes in reusable modules
  and construct a workflow based on a defined set of rules. SciFlow’s workflows could be broken down into
  several components, in order to provide for the user’s natural way of thinking. These workflow components
  would group together several similar modules.</p>
<p>If we take a look at the below example image of the static workflow, we can observe that;</p>

<ul>
  <li>Task 01 belongs to Workflow Component 01</li>
  <li>Task 02 and 03 belong to Workflow Component 02</li>
  <li>Task 04 belongs to Workflow Component 03</li>
</ul>

<p>In a real-world context, let us consider a Data Analytics workflow. Cleaning modules such as, column
  selection and replacing missing values could be packaged under the “Cleaning” workflow component, while mining
  modules such as, linear regression, neural networks etc could be packaged under the “Mining”
   workflow component.</p>
</div>
<br>

<div id="dynamicWF">
<h3>Dynamic Workflows</h3>

<p>A workflow can be either static or dynamic. <b>Static</b> workflows require the flow of the entire workflow to
  be defined well in advance.</p>

<img src="../../../assets/png/componentWF.png" alt="Smiley face" class="center" width="500px">

<p>Dynamic workflows are considered in cases where the workflow structure cannot be determined ahead of time, requiring a part of the workflow to be generated automatically. </p>
<p>Dynamic workflows could be further divided based on the flow of information between the analysis activities in the workflow. A workflow could display dynamicity in two forms;</p>

<ul>
  <li><b>Data flow dependencies :</b> In such instances, the next step of the workflow is determined by the data produced in earlier steps.</li>
  <li><b>Control flow dependencies :</b> The structure of the Big Data analysis solution is determined by the domain expert that orchestrates the data analysis activities in the workflow.</li>
</ul>

<p>SciFlow considers cases of Control Flow Dependent Dynamic workflows. The control thread would make a decision, based on a function written by you, and change the workflow’s path during run-time.</p>
<br>

<img src="../../../assets/png/dynamicWF.png" alt="Smiley face" class="center" width="500px">
</div>
<br>

<div id="composability">
<h3>Composability</h3>

<p>SciFlow’s composability ensures that each workflow task forms a separate, distinct program module. At implementation time each module and its inputs and outputs are well-defined; there is no confusion in the intended interface with other system modules.</p>
<p>At checkout time the integrity of the module is tested independently; there are few scheduling problems in synchronizing the completion of several tasks before checkout can begin. Finally, the system is maintained in modular fashion; system errors and deficiencies can be traced to specific system modules, thus limiting the scope of detailed error searching.</p>
<p>You could write your own workflow modules, depending on the scientific workflow you’re trying to construct. By stinging these modules together in a control thread, you can build a workflow, which fits your requirements.</p>
<p>Such modules could be re-used as well, providing ease for the next time you want to construct a similar application.</p>
<br>

<div id="implicitParallelism">
<h3>Implicit Parallelism</h3>

<p>HPC was initially embraced by research scientists to execute various applications  requiring high computational power that a single computer was unable to provide. HPC ensures a significant performance increase due to faster hardware, higher memory capacity, optimisation, and the use of parallel computing. For instance, by making use of an HPC cluster, one can take advantage of multiple cores by running several instances of a program at once or using a parallel version of the program.</p>
<p>Implicit parallelism is the subset of parallel computing where the programmer is not expected to define how computation is parallelized. SciFlow caters for such implicit parallelism.</p>
<p>With the use of the Python parallel scripting library Parsl, you can easily configure the framework to run parallely. Parsl augments Python with simple, scalable, and flexible constructs for encoding parallelism. You can annotate Python functions to specify opportunities for concurrent execution.</p>
<p>Lets take a look at how Parsl provides parallel execution with a simple example. A program defines two Parsl annotated  functions, app_double and app_sum. It then makes four calls to the app_double app and one call to the app_sum app; these execute concurrently, synchronized by mapped_result variable. The following figure shows the resulting task graph.</p>

<img src="../../../assets/png/MapReduce.jpg" alt="Smiley face" class="center" width="500px">

<p>Within the framework we have provided configurations for running on,</p>

<ul>
  <li>Threads on your local machine</li>
  <li>Cores on your local machine</li>
  <li>On an Ad Hoc Cluster</li>
</ul>

<p>Explore more cloud, cluster and supercomputer configurations on the Parsl website. </p>
<p><a href="https://parsl.readthedocs.io/en/stable/userguide/configuring.html">https://parsl.readthedocs.io/en/stable/userguide/configuring.html</a></p>
<p>There’s absolutely no requirement for you to bother with MPI codes in order to parallelize across an HPC platform. You just need to load the configuration!  </p>
</div>
