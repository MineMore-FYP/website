<div id="parsl-config">

<h3>Step 06 : Configuring Parsl </h3>

<p>Parsl separates code and execution. To do so, it relies on a configuration model to describe the pool of resources to be used for execution (e.g., clusters, clouds, threads, cores).</p>
<p>The configuration provided to Parsl tells Parsl what resources to use to run the Parsl program and apps, and how to use them. Within the SciFlow framework, we have included the following configurations;</p>

<ul>
  <li>Threads on your local machine (local_threads)</li>
  <li>Cores on your local machine (local_htex)</li>
  <li>On an Ad Hoc Cluster (remote_htex)</li>
</ul>


<p>You can simply load these pre-written configurations with the following command in the <b>workflow/parslConfig.py</b> file.</p>


<pre><code>parsl.load(config_name)</code></pre>



<p>The configuration files (in <b>workflow/configs/</b>) need to be slightly amended as per the configurations of your machine/cluster.</p>

<table class="table table-hover table-bordered">
  <thead>
      <tr>
        <th></th>
        <th>Provider, channel and executor</th>
        <th>Parameters to be Changed</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><b>Local Threads</b></td>
        <td>
          <pre class="table-code"><code>from parsl.executors.threads import ThreadPoolExecutor</code></pre>
        </td>
        <td>
          <p>max_threads – Number of threads.</p>
          <p>Default is 2.</p>
        </td>
      </tr>
      <tr>
        <td><b>Local Cores</b></td>
        <td>
          <pre class="table-code"><code>
from parsl.providers import LocalProvider
from parsl.channels import LocalChannel
from parsl.executors import HighThroughputExecutor
          </code></pre>
        </td>
        <td>
          <p>cores_per_worker – cores to be assigned to each worker. </p>
          <p>Oversubscription is possible by setting cores_per_worker < 1.0. Default is 1.</p>

        </td>
      </tr>
      <tr>
        <td><b>Ad-Hoc Cluster</b></td>
        <td>
          <pre class="table-code"><code>
from parsl.providers import AdHocProvider
from parsl.channels import SSHChannel
from parsl.executors import HighThroughputExecutor
from parsl.providers import LocalProvider
from parsl.channels import LocalChannel
          </code></pre>
        </td>
        <td>
          <pre class="table-code"><code>
'username': ‘YOUR USERNAME’
'script_dir': 'YOUR SCRIPT DIRECTORY'
'remote_hostnames': ['REMOTE HOST URL 1','REMOTE HOST URL 2', ...]
          </code></pre>
        </td>
      </tr>
    </tbody>


</table>

<p>Parsl provides numerous other execution environments as well, such as, Amazon Web Services, Google Cloud, Slurm based cluster or supercomputer, Torque/PBS based cluster or supercomputer, Kubernetes cluster etc. If you wish to configure SciFlow for these environments, you could easily do so by writing the configuration within the workflow/configs/ folder and thereafter, loading it in the workflow/parslConfig.py file.</p>
<p>Take a look at <a href="https://parsl.readthedocs.io/en/stable/userguide/configuring.html">Parsl’s documentation</a> for further information on configuration!</p>


</div>
